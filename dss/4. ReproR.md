
# Create reproducible results


## What Problem Does Reproducibility Solve?

What we get

* Transparency
* Data Availability
* Software / Methods Availability
* Improved Transfer of Knowledge

---

## Who Reproduces Research?

* For reproducibility to be effective as a means to check validity, someone needs to do something
  - Re-run the analysis; check results match
  - Check the code for bugs/errors
  - Try alternate approaches; check sensitivity

* The need for someone to do something is inherited from traditional
  notion of replication

* Who is "someone" and what are their goals?

---

## The Story So Far

* Reproducibility brings transparency (wrt code+data) and increased transfer of knowledge
* A lot of discussion about how to get people to share data
* Key question of "can we trust this analysis?" is not addressed by reproducibility
* Reproducibility addresses potential problems long after they’ve occurred ("downstream")
* Secondary analyses are inevitably coloured by the interests/motivations of others

---

## Evidence-based Data Analysis

* Most data analyses involve stringing together many different tools and methods
* Some methods may be standard for a given field, but others are often applied ad hoc
* We should apply thoroughly studied (via statistical research), mutually agreed upon methods to analyze data whenever possible
* There should be evidence to justify the application of a given method

---

* Something that's interesting to you will (hopefully) motivate good
  habits

---

## DON'T: Do Things By Hand



* Editing spreadsheets of data to "clean it up"

  - Removing outliers
  - QA / QC
  - Validating

* Editing tables or figures (e.g. rounding, formatting)

* Downloading data from a web site (clicking links in a web browser)

* Moving data around your computer; splitting / reformatting data files

* "We're just going to do this once...."


Things done by hand need to be precisely documented (this is harder
than it sounds)

---

## DON'T: Point And Click

* Many data processing / statistical analysis packages have graphical
  user interfaces (GUIs)

* GUIs are convenient / intuitive but the actions you take with a GUI
  can be difficult for others to reproduce

* Some GUIs produce a log file or script which includes equivalent
  commands; these can be saved for later examination

* In general, be careful with data analysis software that is highly
  *interactive*; ease of use can sometimes lead to non-reproducible
  analyses

* Other interactive software, such as text editors, are usually fine

---

## DO: Teach a Computer

* If something needs to be done as part of your analysis /
  investigation, try to teach your computer to do it (even if you only
  need to do it once)
  

## DO: Use Some Version Control

* Slow things down

* Add changes in small chunks (don't just do one massive commit)

* Track / tag snapshots; revert to old versions

* Software like GitHub / BitBucket / SourceForge make it easy to
  publish results

---
## DO: Keep Track of Your Software Environment

* If you work on a complex project involving many tools / datasets,
  the software and computing environment can be critical for
  reproducing your analysis

* **Computer architecture**: CPU (Intel, AMD, ARM), GPUs, 

* **Operating system**: Windows, Mac OS, Linux / Unix

* **Software toolchain**: Compilers, interpreters, command shell,
  programming languages (C, Perl, Python, etc.), database backends,
  data analysis software

* **Supporting software / infrastructure**: Libraries, R packages,
  dependencies

* **External dependencies**: Web sites, data repositories, remote
  databases, software repositories

* **Version numbers**: Ideally, for everything (if available)
  

---


## DON'T: Save Output

* Avoid saving data analysis output (tables, figures, summaries,
  processed data, etc.), except perhaps temporarily for efficiency
  purposes.

* If a stray output file cannot be easily connected with the means by
  which it was created, then it is not reproducible.

* Save the data + code that generated the output, rather than the
  output itself

* Intermediate files are okay as long as there is clear documentation
  of how they were created


---

## DO: Set Your Seed

* Random number generators generate pseudo-random numbers based on an
  initial seed (usually a number or set of numbers)

  - In R you can use the `set.seed()` function to set the seed and to
    specify the random number generator to use

* Setting the seed allows for the stream of random numbers to be
  exactly reproducible

* Whenever you generate random numbers for a non-trivial purpose,
  **always set the seed**
  
  
  
  ## DO: Think About the Entire Pipeline

* Data analysis is a lengthy process; it is not just tables / figures
  / reports

* Raw data &rarr; processed data &rarr; analysis &rarr; report

* How you got the end is just as important as the end itself

* The more of the data analysis pipeline you can make reproducible,
  the better for everyone






## Data analysis files

* Data
  * Raw data
  * Processed data
* Figures
  * Exploratory figures
  * Final figures
* R code
  * Raw / unused scripts
  * Final scripts
  * R Markdown files
* Text
  * README files
  * Text of analysis / report


---

## Steps in a data analysis

* Define the question
* Define the ideal data set
* Determine what data you can access
* Obtain the data
* Clean the data
* Exploratory data analysis
* Statistical prediction/modeling
* Interpret results
* Challenge results
* Synthesize/write up results
* Create reproducible code

---

The key challenge in data analysis

"Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you had insufficient information and have to go find some?"

 Dan Myer, Mathematics Educator
 
 
## Define the ideal data set

* The data set may depend on your goal
  * Descriptive - a whole population
  * Exploratory - a random sample with many variables measured
  * Inferential - the right population, randomly sampled
  * Predictive - a training and test data set from the same population
  * Causal - data from a randomized study
  * Mechanistic - data about all components of the system
  

---


## Perform the subsampling
set.seed(3435)
trainIndicator = rbinom(4601, size = 1, prob = 0.5)
table(trainIndicator)
```


## Plots

```r
plot(trainSpam$capitalAve ~ trainSpam$type)
```
```r
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
```

```r
plot(log10(trainSpam[, 1:4] + 1))
```


## Clustering

```r
hCluster = hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
```

## New clustering

```r
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:55] + 1))))
plot(hClusterUpdated)
```

## Interpret results

* Use the appropriate language
  * describes 
  * correlates with/associated with
  * leads to/causes
  * predicts
* Give an explanation
* Interpret coefficients
* Interpret measures of uncertainty

---

## What Have We Learned?

- New York does have very high levels of nickel and vanadium, much
  higher than any other US community

- There is evidence of a positive relationship between Ni
  concentrations and $PM_{10}$ risk

- The strength of this relationship is highly sensitive to the
  observations from New York City

- Most of the information in the data is derived from just 3 observations

---

## Lessons Learned

- Reproducibility of NMMAPS allowed for a secondary analysis (and
  linking with PM chemical constituent data) investigating a novel
  hypothesis (Lippmann *et al.*)

- Reproducibility also allowed for a critique of that new analysis
  and some additional new analysis (Dominici *et al.*)

- Original hypothesis not necessarily invalidated, but evidence not as
  strong as originally suggested (more work should be done)

- Reproducibility allows for the scientific discussion to occur in a
  timely and informed manner

- This is how science works


## Research Papers and LOD:

* People are busy, especially managers and leaders

* Results of data analyses are sometimes presented in oral form, but
  often the first cut is presented via email

* It is often useful to breakdown the results of an analysis into
  different levels of granularity / detail

* Getting responses from busy people: [http://goo.gl/sJDb9V](http://goo.gl/sJDb9V)

---

## Hierarchy of Information: Research Paper

* Title / Author list

* Abstract

* Body / Results

* Supplementary Materials / the gory details

* Code / Data / really gory details

---

## Hierarchy of Information: Email Presentation

* Subject line / Sender info

  - At a minimum; include one
  - Can you summarize findings in one sentence?

* Email body 

  - A brief description of the problem / context; recall what was
    proposed and executed; summarize findings / results; 1&ndash;2
    paragraphs

  - If action needs to be taken as a result of this presentation,
    suggest some options and make them as concrete as possible.

  - If questions need to be addressed, try to make them yes / no

---

## Hierarchy of Information: Email Presentation

* Attachment(s)

  - R Markdown file 
  - knitr report 

  - Stay concise; don't spit out pages of code (because you
    used knitr we know it's available)

* Links to Supplementary Materials

  - Code / Software / Data
  - GitHub repository / Project web site

  
  
  ## Summary: Checklist

* Are we doing good science?

* Was any part of this analysis done by hand?
  - If so, are those parts *precisely* document?
  - Does the documentation match reality?

* Have we taught a computer to do as much as possible (i.e. coded)?

* Are we using a version control system?

* Have we documented our software environment?

* Have we saved any output that we cannot reconstruct from original
  data + code?

* How far back in the analysis pipeline can we go before our results
  are no longer (automatically) reproducible?

  
  
