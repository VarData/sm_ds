## Data science Introduction

## Definition of data

<q>Data are values of qualitative or quantitative variables, belonging to a set of items.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)


## Big or small - you need the right data
<q>The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data... </q> 
[Tukey](http://en.wikipedia.org/wiki/John_Tukey)

**Data Deluge:** The data deluge refers to the situation where the sheer volume of new data being generated is overwhelming the capacity of institutions to manage it and researchers to make use of it.

**Data Dredging:** The use of data mining to uncover patterns in data that can be presented as statistically significant, without first devising a specific hypothesis as to the underlying causality.

#### What do data scientists do?
1. Define the question
2. Define the Ideal data set
3. Determine what data you can access
4. Obtain the data
5. Clean the data
6. Exploratory data analytics
7. Statistical prediction and modelling
8. Interpret results
9. Challenge results
10. Synthesize or writeup results
11. Create reproducible code
12. Distribute results to other people.

One of the main tools for performing these tasks is programming envirnment and Jyupytor notebook, with Anaconda is the best hassle free environment for python.
For R, R studio is the best environment.

---

## 1. Getting and Cleaning data
Once we have the environmetn set up and ready we need to focus on getting data and cleaning it. As the data we get is generally raw data.

#### Connecting and listing databases
```r
ucscDb <- dbConnect(MySQL(), user = "genome", host = "genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb, "show databases;")
dbDisconnect(ucscDb)
result
```

#### Merging data - merge()
```r
mergedData2 <- merge(reviews, solutions, by.x = "solution_id", by.y = "id", 
    all = TRUE)
head(mergedData2[, 1:6], 3)
reviews[1, 1:6]
```


#### Raw versus processed data

__Raw data__
* The original source of the data
* Often hard to use for data analyses
* Data analysis _includes_ processing
* Raw data may only need to be processed once

[http://en.wikipedia.org/wiki/Raw_data](http://en.wikipedia.org/wiki/Raw_data)

__Processed data__
* Data that is ready for analysis
* Processing can include merging, subsetting, transforming, etc.
* There may be standards for processing
* All steps should be recorded 

[http://en.wikipedia.org/wiki/Computer_data_processing](http://en.wikipedia.org/wiki/Computer_data_processing)

---

## 2. Exploratory Analysis 

#### Principles of Analytic Graphics
* Principle 1: Show comparisons
* Principle 2: Show causality, mechanism, explanation
* Principle 3: Show multivariate data
* Principle 4: Integrate multiple modes of evidence
* Principle 5: Describe and document the evidence
* Principle 6: Content is king

#### K-means clustering -  example
```r
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```
---

## 3. Statistics in Data Science

#### Most Common topics
* Basic probability
* Likelihood
* Common distributions
* Asymptotics
* Confidence intervals
* Hypothesis tests
* Power
* Bootstrapping
* Non-parametric tests
* Basic bayesian statistics

### Example

Suppose that the proportion of help calls that get addressed in
a random day by a help line is given by
$$
f(x) = \left\{\begin{array}{ll}
    2 x & \mbox{ for } 1 > x > 0 \\
    0                 & \mbox{ otherwise} 
\end{array} \right. 
$$

Is this a mathematically valid density?

---

## 4. Regresson Analysis for Data Science
* Basic probability
* Likelihood
* Common distributions
* Asymptotics
* Confidence intervals
* Hypothesis tests
* Power
* Bootstrapping
* Non-parametric tests
* Basic bayesian statistics

## 5. Machine Learning
* Prediction study design
* Types of Errors
* Cross validation
* The caret package
* Plotting for prediction
* Preprocessing
* Predicting with regression
* Predicting with trees
* Boosting
* Bagging
* Model blending 
* Forecasting 

---

## Types of Data Science Questions
__In approximate order of difficulty__
* Descriptive
* Exploratory
* Inferential
* Predictive
* Causal
* Mechanistic


#### About descriptive analyses
__Goal__: Describe a set of data

* The first kind of data analysis performed
* Commonly applied to census data
* The description and interpretation are different steps
* Descriptions can usually not be generalized without additional statistical modeling

---

#### About exploratory analysis

__Goal__: Find relationships you didn't know about

* Exploratory models are good for discovering new connections
* They are also useful for defining future studies
* Exploratory analyses are usually not the final say
* Exploratory analyses alone should not be used for generalizing/predicting
* [Correlation does not imply causation](http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation)

---

#### About inferential analysis

__Goal__: Use a relatively small sample of data to say something about a bigger population

* Inference is commonly the goal of statistical models
* Inference involves estimating both the quantity you care about and your uncertainty about your estimate
* Inference depends heavily on both the population and the sampling scheme

---

#### About predictive analysis

__Goal__: To use the data on some objects to predict values for another object

* If $X$ predicts $Y$ it does not mean that $X$ causes $Y$
* Accurate prediction depends heavily on measuring the right variables
* Although there are better and worse prediction models, more data and a simple model [works really well](http://www.youtube.com/watch?v=yvDCzhbjYWs)
* Prediction is very hard, especially about the future [references](http://www.larry.denenberg.com/predictions.html) 

---

#### About causal analysis

__Goal__: To find out what happens to one variable when you make another variable change. 

* Usually randomized studies are required to identify causation
* There are approaches to inferring causation in non-randomized studies, but they are complicated and sensitive to assumptions
* Causal relationships are usually identified as average effects, but may not apply to every individual
* Causal models are usually the "gold standard" for data analysis

---

#### About mechanistic analysis

__Goal__: Understand the exact changes in variables that lead to changes in other variables for individual objects.

* Incredibly hard to infer, except in simple situations
* Usually modeled by a deterministic set of equations (physical/engineering science)
* Generally the random component of the data is measurement error
* If the equations are known but the parameters are not, they may be inferred with data analysis

---

#### Good Data Scientists do good experiments.
* Good experiments
  * Have replication
  * Measure variability
  * Generalize to the problem you care about
  * Are transparent
* Prediction is not inference
  * Both can be important
* Beware data dredging

**Variabity:** Variability is the extent to which data points in a statistical distribution or data set diverge from the average, or mean, value as well as the extent to which these data points differ from each other. There are four commonly used measures of variability: range, mean, variance and standard deviation

**Confounding:** Confounding occurs when the experimental controls do not allow the experimenter to reasonably eliminate plausible      alternative explanations for an observed relationship between independent and dependent variables. For example if we try to experiment shoe size with literarcy, while there is a confounder that is Age.
