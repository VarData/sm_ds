## The four things a data scientist should have

1. The raw data.
2. A tidy data set
3. A code book describing each variable and its values in the tidy data set.
4. An explicit and exact recipe you used to go from 1 -> 2,3.


### The tidy data

1. Each variable you measure should be in one column
2. Each different observation of that variable should be in a different row
3. There should be one table for each "kind" of variable
4. If you have multiple tables, they should include a column in the table that allows them to be linked

_Some other important tips_

* Include a row at the top of each file with variable names. 
* Make variable names human readable AgeAtDiagnosis instead of AgeDx
* In general data should be saved in one file per table.

---
## Downloading Files

```r
if (!file.exists("data")) {
    dir.create("data")
}
```
```r
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl")
list.files("./data")
```

```r
dateDownloaded <- date()
dateDownloaded
```

### Read local flat file

```r
cameraData <- read.table("./data/cameras.csv", sep = ",", header = TRUE)
head(cameraData)
```

### Reading excel files
```r
if(!file.exists("data")){dir.create("data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.xlsx",method="curl")
dateDownloaded <- date()
```
#### read.xlsx(), read.xlsx2() {xlsx package}

```r
library(xlsx)
cameraData <- read.xlsx("./data/cameras.xlsx",sheetIndex=1,header=TRUE)
head(cameraData)
```
#### Reading specific rows and columns

```r
colIndex <- 2:3
rowIndex <- 1:4
cameraDataSubset <- read.xlsx("./data/cameras.xlsx",sheetIndex=1,
                              colIndex=colIndex,rowIndex=rowIndex)
cameraDataSubset
```
### Read XML Files

```r
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
```

#### XPath
* _/node_ Top level node
* _//node_ Node at any level
* _node[@attr-name]_ Node with an attribute name
* _node[@attr-name='bob']_ Node with attribute name attr-name='bob'


```r
xpathSApply(rootNode,"//name",xmlValue)
```

```r
xpathSApply(rootNode,"//price",xmlValue)
```

### Read JSON Files

```r
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
```

## data.table

* Inherets from data.frame
  * All functions that accept data.frame work on data.table
* Written in C so it is much faster
* Much, much faster at subsetting, group, and updating

```r
DT = data.table(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DT,3)
```

```r
DT[,list(mean(x),sum(z))]
```


## Reading mySQL


```install.packages("RMySQL")```

```r
ucscDb <- dbConnect(MySQL(),user="genome", 
                    host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb);
```

```r
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
```
```r
affyMisSmall <- fetch(query,n=10); dbClearResult(query);
```

## Reading data from the web


```r
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
```
#### Using handles

```r
google = handle("http://google.com")
pg1 = GET(handle=google,path="/")
pg2 = GET(handle=google,path="search")
```

## Subsetting and sorting

```r
sort(X$var1)
```

```r
sort(X$var1,decreasing=TRUE)
```

```r
sort(X$var2,na.last=TRUE)
```
```r
X[order(X$var1),]
```
```r
summary(restData)
```
```r
str(restData)
```

```r
quantile(restData$councilDistrict,probs=c(0.5,0.75,0.9))
```

```r
table(restData$zipCode,useNA="ifany")
```

```r
any(is.na(restData$councilDistrict))
```

```r
all(restData$zipCode > 0)
```

```r
colSums(is.na(restData))
```
---

## Creating New variables


```r
restData$zipGroups = cut(restData$zipCode,breaks=quantile(restData$zipCode))
table(restData$zipGroups)
```

## Reshaping Data


```r
library(reshape2)
head(mtcars)
```

```r
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars,id=c("carname","gear","cyl"),measure.vars=c("mpg","hp"))
head(carMelt,n=3)
```
--

## Merging Data

```r
mergedData = merge(reviews,solutions,by.x="solution_id",by.y="id",all=TRUE)
head(mergedData)
```


## Regular expressions

- Regular expressions can be thought of as a combination of literals and _metacharacters_
- To draw an analogy with natural language, think of literal text forming the words of this language, and the metacharacters defining its grammar
- Regular expressions have a rich set of metacharacters

---

When used at the beginning of a character class, the “\^” is also a metacharacter and indicates matching characters NOT in the indicated class

```markdown
[^?.]$
```

Similarly, you can specify a range of letters [a-z] or [a-zA-Z]; notice that the order doesn’t matter

```markdown
^[0-9][a-zA-Z]
```

“.” is used to refer to any character. So

```markdown
9.11
```

This will also match 9/11 or 9:11

“pipe” in the context of regular expressions; instead it translates to “or”; we can use it to combine two expressions, the subexpressions being called alternatives

```markdown
flood|fire
```

Subexpressions are often contained in parentheses to constrain the alternatives

```markdown
^([Gg]ood|[Bb]ad)
```

```markdown
[0-9]+ (.*)[0-9]+
```

