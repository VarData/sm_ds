# Statistics

A Statistic is a number or value

Statistics is science of study of data.

collecting, analyzing, organizing, summarising

Box Plot:- 5 Numbers of Information

Median:- data in the middle of the set
Max
Min
1st Quartile (One quarter of data on graph)
3rd Quartile
Median in case of even number of record
Median 68, 74, 78, 84, 90, 93 (78, 84 => 81)
First Quartile (1/4th of the way) 75

Modified Box Plot

arranging data in meaningful way
Method of

IQR Inter quartile range:- difference between 3rd and first quartile

Modified box plot will be having innerfences:-

Lower Innerfence = min-1.5(IQR)
Upper innerfence = max-1.5(IQR)

Data:- Life expectancy
Summary Measure

Mean vs Median
Mean > Median => 
Median is robust statistics

Trimmed mean 10% data almost equals median and makes mean robust
Mean incorporates the skew-ness characteristics of data.

Spread of data:-
Range of data, or IQR (3rd Quartile - 1st Quartile)

Variance :- sum(Xi -x’)^2/(n-1)
standard deviation :- (Variance)^1/2

Adding all of the deviations




# Generative Model, Discriminitive Model, Conditional Model

A generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal?

A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal.

What is the difference between a Generative and Discriminative Algorithm?

P(Y|X) is discriminitive model, where x is given
P(X, Y) is generative model



# A DATA SCIENTIST'S DIARY
Markov model, Hidden Markov model and Viterbi Algorithm
CHITRAGUPT
In probability theory, a Markov model is a stochastic model used to model randomly changing systems where it is assumed that future states depend only on the current state not on the events that occurred before it (that is, it assumes the Markov property). Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable. For this reason, in the fields of predictive modelling and probabilistic forecasting, it is desirable for a given model to exhibit the Markov property.

In layman's terms what are the differences and similarities between Bayes Networks, Markov Decision Process, and Hidden Markov Models?

A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be aMarkov process with unobserved (hidden) states. An HMM can be presented as the simplest dynamic Bayesian network. The mathematics behind the HMM were developed by L. E. Baum and coworkers.

[1] [2] [3] [4] [5] 
 It is closely related to an earlier work on the optimal nonlinear filtering problem by Ruslan L. Stratonovich,[6]

who was the first to describe the forward-backward procedure.

In simpler Markov models (like a Markov chain), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters. In a hidden Markov model, the state is not directly visible, but the output, dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states. The adjective 'hidden' refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a 'hidden' Markov model even if these parameters are known exactly.

Hidden Markov models are especially known for their application in temporal pattern recognition such as speech,handwriting, gesture recognition,

[7] part-of-speech tagging, musical score following,

[8] partial discharges [9] and bioinformatics.

A hidden Markov model can be considered a generalization of a mixture model where the hidden variables (or latent variables), which control the mixture component to be selected for each observation, are related through a Markov process rather than independent of each other. Recently, hidden Markov models have been generalized to pairwise Markov models and triplet Markov models which allow consideration of more complex data structures

[10] [11] and the modelling of nonstationary data.

[12] [13]

The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states – called the Viterbi path – that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models.

The algorithm has found universal application in decoding the convolutional codes used in both CDMA and GSM digital cellular, dial-up modems, satellite, deep-space communications, and 802.11 wireless LANs. It is now also commonly used in speech recognition, speech synthesis, diarization, [1] keyword spotting,computational linguistics, and bioinformatics. For example, in speech-to-text (speech recognition), the acoustic signal is treated as the observed sequence of events, and a string of text is considered to be the "hidden cause" of the acoustic signal. The Viterbi algorithm finds the most likely string of text given the acoustic signal.




# Linear Rgresssion
CHITRAGUPT
n statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.

Y=Xb+eY=Xb+e
Example. Consider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti. Physics tells us that, ignoring the drag, the relationship can be modeled as

h=b1t1+b2t22+eh=b1t1+b2t22+e

this expression is non linear in time variable but linear in b1 and b2.

(This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.)

Multivariate Linear Regression

http://www.public.iastate.edu/~m...

Multivariate analysis (MVA) is based on the statistical principle of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. In design and analysis, the technique is used to perform trade studies across multiple dimensions while taking into account the effects of all variables on the responses of interest.

Bayesian multivariate linear regression
In statistics, Bayesian multivariate linear regression is a Bayesian approach to multivariate linear regression, i.e.linear regression where the predicted outcome is a vector of correlated random variables rather than a single scalar random variable. A more general treatment of this approach can be found in the article MMSE estimator.

Intuition behind Matrix Multiplication
What is an intuitive explanation of matrix multiplication?

What are some examples of how we use matrix multiplication in practice?

1. Linear Regression - if you want to predict the number of people(continuous response) who will vote for democrats/republicans in each county/city/state etc.,
2. Logistic Regression - if you want to predict the probability that a certain person will vote for a democrat/republican or not.









# Difference between logistic regression and Naive Bayes.
CHITRAGUPT
What is the difference between logistic regression and Naive Bayes?

Murthy Kolluru

Below is the list of 5 major differences between Naïve Bayes and Logistic Regression.

1. Purpose or what class of machine leaning does it solve? 
Both the algorithms can be used for classification of the data. Using these algorithms, you could predict whether a banker can offer a loan to a customer or not or identify given mail is a Spam or ham

2. Algorithm’s Learning mechanism
Naïve Bayes: For the given features (x) and the label y, it estimates a joint probability from the training data. Hence this is a Generative model
Logistic regression: Estimates the probability(y/x) directly from the training data by minimizing error. Hence this is a Discriminative model

3. Model assumptions
Naïve Bayes: Model assumes all the features are conditionally independent .so, if some of the features are dependent on each other (in case of a large feature space), the prediction might be poor.
Logistic regression: It the splits feature space linearly, it works OK even if some of the variables are correlated

4. Model limitations
Naïve Bayes: Works well even with less training data, as the estimates are based on the joint density function
Logistic regression: With the small training data, model estimates may over fit the data

5. Approach to be followed to improve the results
Naïve Bayes: When the training data size is less relative to the features, the information/data on prior probabilities help in improving the results 
Logistic regression: When the training data size is less relative to the features, Lasso and Ridge regression will help in improving the results.
